# -*- coding: utf-8 -*-
"""695_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d5xxNWOgut7Eexk_XFdzWpICM3WYrB7-
"""

import pandas as pd
import numpy as np
import tensorflow as tf

data = pd.read_csv("training.csv")
pred_final = pd.read_csv("final_predicatio1n(2).csv")

data.head(5)

data.describe()

data.info()

data['Location_Sales_Volume_Actual'] = data['Location_Sales_Volume_Actual'].str.replace(',', '')
data['Location_Sales_Volume_Actual'] = data['Location_Sales_Volume_Actual'].str.replace('$', '')
data['Location_Sales_Volume_Actual'] = data['Location_Sales_Volume_Actual'].astype(int)

sales = data.Location_Sales_Volume_Actual

sales.shape

"""### **1. Linear Regression model**"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

reg = LinearRegression()
X = data.drop(['Location_Sales_Volume_Actual'], axis=1)
y = data[['Location_Sales_Volume_Actual']]

#Train-Test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#Train the model
reg.fit(X_train, y_train)

for idx, col_name in enumerate(X.columns):
    print("The coefficient for {} is {}".format(col_name, reg.coef_[0][idx]))

import statsmodels.api as sm
from scipy import stats

X2 = sm.add_constant(X)
est = sm.OLS(y, X2)
est2 = est.fit()
print(est2.summary())

import matplotlib.pyplot as plt
#predictedStr = reg.predict(data.iloc[:,0:-1])
y_pred = reg.predict(X_test)
plt.scatter(x= y_pred, y=y_test) 
plt.title("Linear Regression Predicted Amount of Sales vs Actual Sales")
plt.xlabel("Predicted Amount of Sales")
plt.ylabel("Actual Amount of Sales")

X_final = pred_final.drop(['Location_Sales_Volume_Actual'], axis=1)
y_pred_final = reg.predict(X_final)
pred_final['Location_Sales_Volume_Actual'] = y_pred_final
print(pred_final)

"""#### **Linear Regression Model Result**"""

from sklearn.metrics import mean_absolute_error
errors = mean_absolute_error(y_test, y_pred)
print('Mean absolute Error of linear regression model:', round(np.mean(errors), 2))

"""### **2. Random Forest Model**"""

# Import the model we are using
from sklearn.ensemble import RandomForestRegressor
# Instantiate model with 1000 decision trees
rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)
# Train the model on training data
rf.fit(X_train, y_train);

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt

def rf_feat_importance(m, df):
    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}
                       ).sort_values('imp', ascending=False)
# %time fi = rf_feat_importance(rf, X_train)
fi[:10]

pi = fi[:10]
print(pi)
df = pd.DataFrame(pi,columns=['cols','imp'])
df.plot.barh(x ='cols', y='imp').invert_yaxis()
plt.show()

"""#### **Random Forest Model Result**"""

# Use the forest's predict method on the test data
y_pred2 = rf.predict(X_test)
# Calculate the absolute errors
errors2 = mean_absolute_error(y_test, y_pred2)
# Print out the mean absolute error (mae)
print('Mean absolute Error of random forest model:', round(np.mean(errors2), 2))

import matplotlib.pyplot as plt
#predictedStr = reg.predict(data.iloc[:,0:-1])
plt.scatter(x= y_pred2, y=y_test) 
plt.title("Random Forest Predicted Amount of Sales vs Actual Sales")
plt.xlabel("Predicted Amount of Sales")
plt.ylabel("Actual Amount of Sales")

"""### Neural Network"""

X = data.drop(['Location_Sales_Volume_Actual'], axis=1)
y = data[['Location_Sales_Volume_Actual']]

#print (data.columns.intersection(lst))
#Index(['A', 'B'], dtype='object')
selected = pi['cols'].tolist()
data2 = data[data.columns.intersection(selected)]
print(data2)

from keras.utils import plot_model
from keras import models
from keras import layers
import matplotlib.pyplot as plt
from IPython.display import Image
from keras.utils.vis_utils import model_to_dot

model = models.Sequential()
#model.add(layers.Dropout(0.5))
model.add(layers.Dense(128, activation='relu', input_shape=(96,)))#,name="d1"))
model.add(layers.Dropout(0.3))
model.add(layers.Dense(64, activation='relu'))#,name="d2"))
model.add(layers.Dropout(0.3))
model.add(layers.Dense(1,name="output"))

# Display model graph
G = model_to_dot (model)
Image (G.create (prog = "dot", format = "png"))

from keras.layers import Dense

model2 = models.Sequential()
model2.add(Dense(64, input_shape=(10,), kernel_initializer='normal'))
model2.add(layers.Dropout(0.3))
model2.add(Dense(25))
model2.add(layers.Dropout(0.3))
model2.add(Dense(25))
model2.add(Dense(1, kernel_initializer='normal'))
G = model_to_dot (model2, dpi=76)
Image (G.create (prog = "dot", format = "png"))

from keras.layers import Dense

model3 = models.Sequential()
model3.add(Dense(50, input_shape=(10,)))
model3.add(Dense(10))
model3.add(Dense(10))
model3.add(Dense(1))
G = model_to_dot (model3)
Image (G.create (prog = "dot", format = "png"))

model4 = Sequential()
model4.add(Dense(12, input_dim=10, kernel_initializer='normal', activation='relu'))
model4.add(Dense(8, activation='relu'))
model4.add(Dense(1, activation='linear'))
model4.summary()

G = model_to_dot (model4)
Image (G.create (prog = "dot", format = "png"))

# Split training dataset into train, test and validation subsets
import numpy as np
from sklearn.model_selection import train_test_split

y = np.asarray(y)
x_train2, x_test2, y_train2, y_test2 = train_test_split(data2, y, test_size=0.2, random_state=1)
x_train2, x_val2, y_train2, y_val2 = train_test_split(x_train2, y_train2, test_size=0.2, random_state=1)
print("x train's shape is: ",x_train2.shape)

print(x_test2)

# Define fit_model function
# Train with appropriate hyperparameters, including optimizer, loss function, epochs, and batch_size

def fit_model(model, x_train, y_train, x_val, y_val, x_test, y_test):
    '''Input: model, x_train, y_train, x_val, y_val, x_test, y_test'''
    '''Output: training history'''
    
    history = None

    opt = tf.keras.optimizers.Adam(0.01)

    model.compile(loss='mean_absolute_error',
    optimizer=opt,)
    #metrics=['accuracy'])
    
    history = model.fit(x_train,
    y_train,
    epochs=200,
    batch_size=5,
    validation_data=(x_val, y_val))
    
    test_loss = model.evaluate(x_test, y_test)
    #test_loss, test_acc = model.evaluate(x_test, y_test)
    print('Test loss:', test_loss)
    #print('Accuracy:', test_acc)
   
    return history

model_a = models.Sequential()
model_a.add(layers.Dense(128, activation='relu', input_shape=(10,)))
model_a.add(layers.Dropout(0.5))
#model_a.add(layers.Dense(8, activation='relu'))
#model_a.add(layers.Dropout(0.5))
model_a.add(layers.Dense(1))

# Display model graph
#G = model_to_dot (model_a)
#Image (G.create (prog = "dot", format = "png"))

import keras
# Fit model A
hist_a = fit_model(model_a, x_train2, y_train2,x_val2, y_val2,x_test2, y_test2)

# Fit the 2nd model, compare with model A
hist_B = fit_model(model, x_train2, y_train2,x_val2, y_val2,x_test2, y_test2)

opt = tf.keras.optimizers.Adam(learning_rate=0.1)
model2.compile(loss='mse', optimizer=opt)
history = model2.fit(x_train2, y_train2, epochs=100)

import keras
# Fit model C
hist_c = fit_model(model2, x_train2, y_train2,x_val2, y_val2,x_test2, y_test2)

import keras
# Fit model D
hist_d = fit_model(model3, x_train2, y_train2,x_val2, y_val2,x_test2, y_test2)

import keras
# Fit model E
hist_e = fit_model(model4, x_train2, y_train2,x_val2, y_val2,x_test2, y_test2)

y_pred5 = model2.predict(x_test2)
errors5 = mean_absolute_error(y_test2, y_pred5)
print('Mean absolute Error of Neural Network model:', round(np.mean(errors5), 2))

y_pred6 = model3.predict(x_test2)
errors6 = mean_absolute_error(y_test2, y_pred6)
print('Mean absolute Error of Neural Network model:', round(np.mean(errors6), 2))

y_pred7 = model4.predict(x_test2)
errors7 = mean_absolute_error(y_test2, y_pred7)
print('Mean absolute Error of Neural Network model:', round(np.mean(errors7), 2))

import matplotlib.pyplot as plt
#predictedStr = reg.predict(data.iloc[:,0:-1])
plt.scatter(x= y_pred4, y=y_test2) 
plt.title("Predicted Amount of Sales vs Actual Sales")
plt.xlabel("Predicted Amount of Sales")
plt.ylabel("Actual Amount of Sales")

import matplotlib.pyplot as plt
#predictedStr = reg.predict(data.iloc[:,0:-1])
plt.scatter(x= y_pred6, y=y_test2) 
plt.title("Predicted Amount of Sales vs Actual Sales")
plt.xlabel("Predicted Amount of Sales")
plt.ylabel("Actual Amount of Sales")

y_pred3 = model.predict(X_test)
errors3 = mean_absolute_error(y_test, y_pred3)
print('Mean absolute Error of Neural Network model:', round(np.mean(errors3), 2))

from sklearn.metrics import mean_absolute_error
y_pred4 = model2.predict(X_test)
errors4 = mean_absolute_error(y_test, y_pred4)
print('Mean absolute Error of Neural Network model:', round(np.mean(errors4), 2))

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt

def rf_feat_importance(m, df):
    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}
                       ).sort_values('imp', ascending=False)
# %time fi = rf_feat_importance(rf, X_train)
fi[:10]

pi = fi[:10]
df = pd.DataFrame(pi,columns=['cols','imp'])
df.plot.barh(x ='cols', y='imp').invert_yaxis()
plt.show()

from sklearn.ensemble import AdaBoostClassifier
adaboost = AdaBoostClassifier(n_estimators=5000) 
adaboost.fit(X_train, y_train)

from sklearn.ensemble import AdaBoostClassifier
adaboost = AdaBoostClassifier(n_estimators=5000) 
adaboost.fit(x_train2, y_train2)

adaboost_score = adaboost.score(X_test,y_test)
print('Adaboost score:', round(adaboost_score*100, 3))

adaboost_predicted = adaboost.predict(X_test)
errors_ada = mean_absolute_error(y_test, adaboost_predicted)
print('Mean absolute Error of Adaboost:', round(np.mean(errors_ada), 2))

adaboost_predicted = adaboost.predict(x_test2)
errors_ada = mean_absolute_error(y_test2, adaboost_predicted)
print('Mean absolute Error of Adaboost:', round(np.mean(errors_ada), 2))

import matplotlib.pyplot as plt
#predictedStr = reg.predict(data.iloc[:,0:-1])
plt.scatter(x= adaboost_predicted, y=y_test2) 
plt.title("Adaboost Predicted Amount of Sales vs Actual Sales")
plt.xlabel("Predicted Amount of Sales")
plt.ylabel("Actual Amount of Sales")

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
plt.style.use('ggplot')

x = ['Neural I', 'Neural II', 'Neural III','Neural IV']
MAE = [20921643.0, 8155980.32, 4686364.06, 4722563.74]

x_pos = [i for i, _ in enumerate(x)]

fig_dims = (8, 6)
fig, ax = plt.subplots(figsize=fig_dims)
splot = sns.barplot(x_pos, MAE, ax=ax, palette="Blues_d")
for p in splot.patches:
    splot.annotate(format(p.get_height(), '.1f'), 
                   (p.get_x() + p.get_width() / 2., p.get_height()), 
                   ha = 'center', va = 'center', 
                   xytext = (0, 9), 
                   textcoords = 'offset points')
plt.xlabel("Model")
plt.ylabel("MAE")
plt.title("MAE of Neural network models")

plt.xticks(x_pos, x)

plt.show()